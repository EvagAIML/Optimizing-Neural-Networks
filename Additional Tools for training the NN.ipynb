{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjT7+fzdrj5WkZAdlDGSyU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Fucntion for calculating the threshold using precision_recall_curve"],"metadata":{"id":"TrHgrv9x2p_b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og2veU7A2o9L"},"outputs":[],"source":["from sklearn.metrics import precision_recall_curve, average_precision_score, auc\n","\n","def Find_Best_Threshold(y_val, y_val_pred):\n","\n","\n","    # Calculate ROC curve\n","    fpr, tpr, roc_thresholds = roc_curve(y_val, y_val_pred)\n","    roc_auc = auc(fpr, tpr)\n","\n","    # Calculate Precision-Recall curve\n","    precision, recall, _ = precision_recall_curve(y_val, y_val_pred)\n","    pr_auc = auc(recall, precision)\n","\n","    # Plot ROC curve\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","\n","    # Plot Precision-Recall curve\n","    plt.subplot(1, 2, 2)\n","    plt.plot(recall, precision, color='red', label=f'PR curve (area = {pr_auc:.2f})')\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title('Precision-Recall (PR) Curve')\n","    plt.legend(loc=\"lower left\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","\n","    # You can also select the best threshold based on the ROC curve\n","    best_roc_threshold_idx = np.argmax(tpr - fpr)\n","    best_roc_threshold = roc_thresholds[best_roc_threshold_idx]\n","\n","    return best_roc_threshold\n"]},{"cell_type":"markdown","source":["### Early Stopping and Reduce Learning Rate"],"metadata":{"id":"N9UBSRqg2_ic"}},{"cell_type":"code","source":["#Initializing the neural network\n","model_0 = Sequential()\n","#Adding the input layer with 64 neurons and relu as activation function\n","model_0.add(Dense(64,activation='relu',input_dim = X_train.shape[1]))\n","# Adding the first hidden layer with 32 neurons with relu as activation functions\n","model_0.add(Dense(32,activation='relu'))\n","# Adding the output layer\n","model_0.add(Dense(1, activation = 'sigmoid'))\n","#Compiling the ANN with SGD optimizer.\n","optimizer = tf.keras.optimizers.SGD(0.001)\n","# Complining the model with binary cross entropy as loss function and accuracy as metrics\n","model_0.compile(loss='binary_crossentropy',optimizer=optimizer"],"metadata":{"id":"vjagSVSx3T7B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","early_stopping = EarlyStopping(\n","    monitor='val_recall',    # Focus on recall for early stopping\n","    patience=5,             # Number of epochs with no improvement\n","    mode='max',              # We want to maximize recall,\n","    start_from_epoch=20,     # Start early stopping from epoch 19\n","    restore_best_weights=True,\n","    min_delta=0.0001, #default is zero\n",")\n","\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='val_loss',\n","    factor=0.2,       # Reduce LR by a factor of 0.2\n","    patience=5,       # Reduce LR after 5 epochs with no improvement\n","    min_lr=0.00001,   # Lower bound for LR\n","    min_delta=0.0001  # default is 0.0001\n",")\n","\n"],"metadata":{"id":"Kasb5XmR2-lU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fitting the ANN with batch_size = 32 and 100 epochs\n","history_0 = model_0.fit(\n","    X_train,y_train,\n","    batch_size=32,\n","    validation_data=(X_val,y_val),\n","    epochs=100,\n","    verbose=1,\n","    callbacks=[early_stopping, ReduceLROnPlateau],\n",")"],"metadata":{"id":"JTXA9U3e3XcL"},"execution_count":null,"outputs":[]}]}