{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOEK+uQFx2nM8hUUlM+WOaU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# The Xavier initialization"],"metadata":{"id":"VkJLHf4YUaag"}},{"cell_type":"markdown","source":["also known as Glorot initialization (named after Xavier Glorot who proposed it), is a weight initialization technique used to help alleviate the problem of vanishing and exploding gradients in deep neural networks with sigmoid and tanh activation functions. It's particularly effective for layers in a deep network where each neuron's output variance is not too dissimilar to its input variance across layers, helping to keep the gradient magnitudes reasonable throughout the depth of the network."],"metadata":{"id":"FEIRRgxGUfTI"}},{"cell_type":"markdown","source":["## Principle of Xavier Initialization"],"metadata":{"id":"j9lNXyPfUm6u"}},{"cell_type":"markdown","source":["The main idea behind Xavier initialization is to keep the scale of the gradients roughly the same in all layers. During the training of a deep network, if the weights are too small, the signal shrinks as it passes through each layer until it's too tiny to be useful. If the weights are too large, the signal grows until it becomes too massive and results in numerical instability.\n","\n","Xavier initialization specifically addresses these issues by considering the number of input and output neurons associated with a specific layer and initializing the weights to maintain a variance that allows for an appropriate flow of gradients."],"metadata":{"id":"PP5xsS1YUphY"}},{"cell_type":"markdown","source":["Xavier initialization sets a layer's weights **ùëä** randomly drawn from a distribution with zero mean and a specific variance"],"metadata":{"id":"_kFb0OmqU0ZA"}},{"cell_type":"markdown","source":["## When to Use Xavier Initialization\n","\n","* **Activation Functions:** It is generally recommended for networks using the tanh or sigmoid activation functions because these activations can exacerbate the vanishing gradients problem due to their mathematical properties.\n","* **Not Ideal for ReLU:** For networks using ReLU activations, **He** initialization (a similar approach that considers the rectifier's characteristics) is generally preferred. Xavier can lead to weights that are too small for ReLU neurons, potentially resulting in dead neurons during training."],"metadata":{"id":"EhcXusTCVTON"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# For a dense layer with sigmoid activation\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(256, activation='sigmoid',\n","                          kernel_initializer=tf.keras.initializers.GlorotUniform(), # Xavier Uniform\n","                          input_shape=(input_dim,))\n","])\n"],"metadata":{"id":"9tSevmX2Vo8C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Momentum in SGD"],"metadata":{"id":"pVtUBW-nYUIT"}},{"cell_type":"markdown","source":["SGD with momentum considers the past gradients to determine the direction of the new update. Essentially, it adds a fraction of the update vector from the previous step to the current step's update, creating a smoother and more stable convergence to the minimum of the loss function.\n","\n","Momentum is designed to accelerate convergence by combining:\n","\n","A fraction of the previous update (scaled by the momentum parameter).\n","The current gradient update.\n","The formula for velocity is:\n","\n","$v_t = momentum.v_t-\\eta.g_t$\n","\n","Where:\n","* $v_t$: Velocity at time $t$\n","* $momentum$: The momentum parameter (typically $0 ‚â§ momentum < 1$)\n","* $\\eta$: Learning Rate\n","* $g_t$: Gradient at time $t$\n"],"metadata":{"id":"j4oQ82qNYaAF"}},{"cell_type":"markdown","source":["## Benefits of Using Momentum\n","* **Faster Convergence:** Momentum can lead to faster convergence by accelerating gradient descent in the right direction, thus reducing the oscillations.\n","* **Smoothing Effect:** It helps to smooth out the steps of SGD, which is beneficial when dealing with noisy data or gradients.\n","* **Escape from Plateaus:** In scenarios where the algorithm encounters flat areas (plateaus) or local minima, momentum can help to escape and continue learning."],"metadata":{"id":"453N0fwEYfEO"}},{"cell_type":"markdown","source":["### When momentum is not recommended?\n","\n","* Highly noisy gradients.\n","* Extremely flat loss landscapes.\n","* Dynamically changing objectives.\n","* Small datasets.\n","* Poorly tuned learning rates.\n","* Resource-constrained environments.\n","* Shallow or simple models.\n"],"metadata":{"id":"kL-X1fd8OxGL"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Define an SGD optimizer with momentum\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n","\n","# Configure the optimizer in a Keras model\n","model.compile(optimizer=optimizer, loss='mean_squared_error')\n"],"metadata":{"id":"Cfnjbn5sYofW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ADAM Optimizer"],"metadata":{"id":"yv4KGXbBajt7"}},{"cell_type":"markdown","source":["The ADAM optimizer (short for \"Adaptive Moment Estimation\") is a widely used method in training neural networks, particularly effective due to its handling of learning rates for individual parameters and its efficient use of computational resources. ADAM combines ideas from two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp), providing the benefits of both methods in handling sparse gradients on noisy problems."],"metadata":{"id":"sSdFOPyMall1"}},{"cell_type":"markdown","source":["## How ADAM Works\n","ADAM maintains two separate estimates for each parameter:\n","\n","* **First Moment (the mean)** - Essentially an exponentially decaying average of past gradients.\n","* **Second Moment (the uncentered variance)** - An exponentially decaying average of past squared gradients."],"metadata":{"id":"BlUQfOdNannH"}},{"cell_type":"markdown","source":["## Benefits of Using ADAM\n","* **Adaptive Learning Rate:** Individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients provide robustness to initial learning rate choices.\n","* **Efficiency:** Computationally efficient with little memory requirements relative to the capability it offers.\n","* **Well-suited for Problems:** Performs well on problems with large datasets or many parameters or when the objective function is very noisy.\n","\n","* Bias Correction: Includes bias corrections to the first and second moments, which help the moments to converge more rapidly at the beginning of training."],"metadata":{"id":"zP5X1mtmawzq"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Create a model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(64, activation='relu', input_shape=(100,)),\n","    tf.keras.layers.Dense(1)\n","])\n","\n","# Compile the model with ADAM optimizer\n","model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","# Train the model\n","model.fit(x_train, y_train, epochs=10)\n"],"metadata":{"id":"_kFu6YIrauV5"},"execution_count":null,"outputs":[]}]}